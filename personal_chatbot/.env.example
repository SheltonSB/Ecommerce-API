# Backend: ollama or llama_cpp
BACKEND=ollama

# Ollama settings
OLLAMA_BASE_URL=http://localhost:11434
MODEL_NAME=llama3.1

# llama.cpp settings (GGUF model)
MODEL_PATH=/absolute/path/to/model.gguf
N_CTX=4096
N_GPU_LAYERS=0

# Generation
TEMPERATURE=0.7
MAX_TOKENS=512

# Memory
MEMORY_MODE=heuristic
MEMORY_LAST_N=12
MEMORY_NOTES_LIMIT=6

# Web
WEB_HOST=127.0.0.1
WEB_PORT=8000

# Optional
SYSTEM_PROMPT=
